{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77e4582",
   "metadata": {},
   "source": [
    "# Lung Cancer Detection using Convolutional Neural Network and Open Ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882c6b9",
   "metadata": {},
   "source": [
    "**Importing Libraries:**\n",
    "\n",
    "numpy (as np): A library for numerical operations and array manipulation.\n",
    "pandas (as pd): A library for data manipulation and analysis.\n",
    "matplotlib.pyplot (as plt): Used for data visualization, including creating plots and charts.\n",
    "PIL (Python Imaging Library): A library for working with image files.\n",
    "glob: Used for file path manipulation and searching for files in directories.\n",
    "sklearn: The scikit-learn library for machine learning, which provides various tools for classification, regression, clustering, and more.\n",
    "cv2 (OpenCV): An open-source computer vision library used for image processing.\n",
    "gc (garbage collector): Python's built-in module for controlling the automatic garbage collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be1c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import cv2\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc354e",
   "metadata": {},
   "source": [
    "**Importing TensorFlow and Keras:**\n",
    "\n",
    "tensorflow (as tf): A popular open-source machine learning framework developed by Google.\n",
    "keras: An integrated deep learning framework that comes with TensorFlow and is used for building and training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f144283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a326a",
   "metadata": {},
   "source": [
    "**Data Path and Directory Structure:**\n",
    "\n",
    "The code begins by defining the path to a dataset stored in a ZIP file. It extracts the dataset using the ZipFile module, and the dataset is organized into a directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca218eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has been extracted.\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# Extract the dataset from the ZIP file\n",
    "data_path = r\"C:\\Users\\chowd\\Downloads\\archive.zip\"\n",
    "\n",
    "with ZipFile(data_path, 'r') as zip:\n",
    "    zip.extractall()\n",
    "    print('The dataset has been extracted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a30294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and classes\n",
    "path = 'lung_colon_image_set/lung_image_sets'\n",
    "classes = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a9904c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "SPLIT = 0.2\n",
    "EPOCHS = 10  # Increase the number of epochs\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc132f5a",
   "metadata": {},
   "source": [
    "**Data Preprocessing and Augmentation:**\n",
    "\n",
    "It sets parameters for image preprocessing and augmentation using ImageDataGenerator. This includes rescaling pixel values to the range [0, 1], as well as defining data augmentation techniques like rotation, shifting, and flipping to increase the diversity of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db1b1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93daa58",
   "metadata": {},
   "source": [
    "**Loading and Preprocessing Images:**\n",
    "\n",
    "The code loads and preprocesses images from the dataset. It iterates through the subdirectories in the dataset directory and reads images using OpenCV (cv2). The images are resized to a specified size (defined by IMG_SIZE) and added to the X list. The corresponding labels (class indices) are added to the Y list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1665c7",
   "metadata": {},
   "source": [
    "**One-Hot Encoding:**\n",
    "\n",
    "The labels (Y) are one-hot encoded using pd.get_dummies. This converts categorical labels into binary vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845c5bf",
   "metadata": {},
   "source": [
    "**Train-Test Split:**\n",
    "\n",
    "The dataset is split into training and validation sets using train_test_split from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "654e28ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 256, 256, 3) (3000, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i, cat in enumerate(classes):\n",
    "    images = glob(f'{path}/{cat}/*.jpeg')\n",
    "\n",
    "    for image in images:\n",
    "        img = cv2.imread(image)\n",
    "        X.append(cv2.resize(img, (IMG_SIZE, IMG_SIZE)))\n",
    "        Y.append(i)\n",
    "\n",
    "\n",
    "X = np.asarray(X)\n",
    "one_hot_encoded_Y = pd.get_dummies(Y).values\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, one_hot_encoded_Y,\n",
    "                                                  test_size=SPLIT,\n",
    "                                                  random_state=2022)\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca96c1cf",
   "metadata": {},
   "source": [
    "**Building a Convolutional Neural Network (CNN):**\n",
    "\n",
    "A CNN model is defined using Keras. The model consists of convolutional layers, max-pooling layers, fully connected layers, and batch normalization layers. It is compiled with an optimizer, loss function, and evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f0eacc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a more complex CNN model\n",
    "model = keras.models.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3), padding='same'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(len(classes), activation='softmax')  # Output neurons equal to the number of classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07bdc1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba16362",
   "metadata": {},
   "source": [
    "**Model Training and Callbacks:**\n",
    "\n",
    "The model is trained using the training data. Training is monitored using callbacks like early stopping (EarlyStopping) and learning rate reduction (ReduceLROnPlateau) to improve training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "585c79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and learning rate scheduler callbacks\n",
    "es = keras.callbacks.EarlyStopping(patience=3, monitor='val_accuracy', restore_best_weights=True)\n",
    "lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e1aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=[es, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250743e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164eac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "Y_pred = model.predict(X_val)\n",
    "Y_val = np.argmax(Y_val, axis=1)\n",
    "Y_pred = np.argmax(Y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aeb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(Y_val, Y_pred))\n",
    "print(metrics.classification_report(Y_val, Y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "250d8802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description for lungscc92.jpg: \n",
      "\n",
      "\n",
      "\n",
      "The image shows two lungs, one on the left and one on the right. They are surrounded by blood vessels and nerves. The left lung is smaller than the right lung.\n",
      "Description for lungscc91.jpg: \n",
      "\n",
      "The image is a closeup of a human lungs with cancerous growths.\n",
      "Description for lungscc93.jpg: \n",
      "\n",
      "This image is of a pair of lungs, viewed from the front. The left lung is mostly obscured by the heart, which is located in the center of the chest. The right lung is visible and appears to be healthy. There are numerous\n"
     ]
    }
   ],
   "source": [
    "# GPT-3 Integration\n",
    "import openai\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual API key\n",
    "api_key = 'YOUR_API_KEY'\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Example of using GPT-3 to generate image descriptions\n",
    "def generate_image_descriptions(images, max_tokens=50):\n",
    "    descriptions = []\n",
    "    for image in images:\n",
    "        prompt = f\"Describe the image: '{image}'\"\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        generated_description = response.choices[0].text\n",
    "        descriptions.append(generated_description)\n",
    "    return descriptions\n",
    "\n",
    "# Example usage\n",
    "image_files = ['lungscc92.jpg', 'lungscc91.jpg', 'lungscc93.jpg']\n",
    "descriptions = generate_image_descriptions(image_files)\n",
    "\n",
    "for i, description in enumerate(descriptions):\n",
    "    print(f\"Description for {image_files[i]}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db215e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
